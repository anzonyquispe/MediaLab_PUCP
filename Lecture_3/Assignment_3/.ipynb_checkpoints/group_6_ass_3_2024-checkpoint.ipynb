{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the `REC0111.sav`, `RE223132.sav` and `RE516171.sav` files and their variables and values labels from this path `\"../../_data/endes/2019\"`. The name of imported files should be named as `rec_1`, `rec_2` and `rec_3` for files `REC0111.sav`, `RE223132.sav` and `RE516171.sav` respectively. The name of the variable and value labels should be `var_labels1` and `value_labels1` for `rec1`, `var_labels2` and `value_labels2` for `rec2`, and `var_labels3` and `value_labels3` for `rec3`. **Hint: See the section 3.3.4 of [the lecture 3](https://github.com/alexanderquispe/Diplomado_PUCP/blob/main/Lecture_3/Lecture_3.ipynb)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyreadstat\n",
      "  Obtaining dependency information for pyreadstat from https://files.pythonhosted.org/packages/b4/16/20564e8afcdf0c36519a503fdfca86b886bda2b8dc436333e8e8b8e8d8f4/pyreadstat-1.2.6-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading pyreadstat-1.2.6-cp311-cp311-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\anauz\\anaconda3\\lib\\site-packages (from pyreadstat) (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\anauz\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anauz\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\anauz\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\anauz\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anauz\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->pyreadstat) (1.16.0)\n",
      "Downloading pyreadstat-1.2.6-cp311-cp311-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/2.4 MB 487.6 kB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.1/2.4 MB 871.5 kB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.1/2.4 MB 774.0 kB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.2/2.4 MB 841.6 kB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 0.3/2.4 MB 1.0 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 0.4/2.4 MB 1.2 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.5/2.4 MB 1.4 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.6/2.4 MB 1.5 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.7/2.4 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.8/2.4 MB 1.6 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.9/2.4 MB 1.6 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 1.0/2.4 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.1/2.4 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.2/2.4 MB 1.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.3/2.4 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.4/2.4 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.5/2.4 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.6/2.4 MB 1.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.6/2.4 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.8/2.4 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.8/2.4 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.9/2.4 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.1/2.4 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.2/2.4 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.3/2.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.4/2.4 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 1.8 MB/s eta 0:00:00\n",
      "Installing collected packages: pyreadstat\n",
      "Successfully installed pyreadstat-1.2.6\n"
     ]
    }
   ],
   "source": [
    "!pip install pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"../../_data/endes/2019/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"REC0111.sav\", \"RE223132.sav\", \"RE516171.sav\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_1, meta_1 = pyreadstat.read_sav(f\"{folder}{files[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_2, meta_2 = pyreadstat.read_sav(f\"{folder}{files[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_3, meta_3 = pyreadstat.read_sav(f\"{folder}{files[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_labels1 = meta_1.column_names_to_labels\n",
    "value_labels1 = meta_1.variable_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_labels2 = meta_2.column_names_to_labels\n",
    "value_labels2 = meta_2.variable_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_labels3 = meta_3.column_names_to_labels\n",
    "value_labels3 = meta_3.variable_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ID1             HHID              CASEID  V001  V002  V003  V004  \\\n",
      "0  2019.0        000100201        000100201  2   1.0   2.0   2.0   1.0   \n",
      "1  2019.0        000100201        000100201  3   1.0   2.0   3.0   1.0   \n",
      "2  2019.0        000102801        000102801  2   1.0  28.0   2.0   1.0   \n",
      "3  2019.0        000102801        000102801  6   1.0  28.0   6.0   1.0   \n",
      "4  2019.0        000104801        000104801  2   1.0  48.0   2.0   1.0   \n",
      "\n",
      "     V007    V008  V009  ...  QD333_4  QD333_5  QD333_6  UBIGEO  V022  \\\n",
      "0  2019.0  1434.0   4.0  ...      2.0      2.0      2.0  010101   3.0   \n",
      "1  2019.0  1434.0   1.0  ...      2.0      2.0      2.0  010101   3.0   \n",
      "2  2019.0  1434.0   6.0  ...      2.0      2.0      2.0  010101   3.0   \n",
      "3  2019.0  1434.0   3.0  ...      2.0      2.0      2.0  010101   3.0   \n",
      "4  2019.0  1434.0   5.0  ...      2.0      2.0      2.0  010101   3.0   \n",
      "\n",
      "       V005  V190      V191  mujeres12a49  NCONGLOME  \n",
      "0  154803.0   4.0  1.234450           2.0     7065.0  \n",
      "1  154803.0   4.0  1.234450           0.0     7065.0  \n",
      "2  154803.0   4.0  1.295611           2.0     7065.0  \n",
      "3  154803.0   4.0  1.295611           2.0     7065.0  \n",
      "4  154803.0   2.0 -0.256431           2.0     7065.0  \n",
      "\n",
      "[5 rows x 105 columns]\n",
      "      ID1              CASEID  V201  V202  V203  V204  V205  V206  V207  V208  \\\n",
      "0  2019.0        000100201  2   2.0   1.0   1.0   0.0   0.0   0.0   0.0   1.0   \n",
      "1  2019.0        000100201  3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "2  2019.0        000102801  2   3.0   1.0   2.0   0.0   0.0   0.0   0.0   2.0   \n",
      "3  2019.0        000102801  6   3.0   1.0   0.0   2.0   0.0   0.0   0.0   0.0   \n",
      "4  2019.0        000104801  2   3.0   1.0   2.0   0.0   0.0   0.0   0.0   1.0   \n",
      "\n",
      "   ...  V307_09  V307_10  V307_11  V307_12  V307_13  V307_14  V307_15  \\\n",
      "0  ...      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
      "1  ...      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
      "2  ...      0.0      0.0      NaN      NaN      NaN      NaN      NaN   \n",
      "3  ...      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
      "4  ...      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
      "\n",
      "   V307_16  QI302_05A  QI302_05B  \n",
      "0      0.0        1.0        2.0  \n",
      "1      NaN        NaN        NaN  \n",
      "2      0.0        2.0        1.0  \n",
      "3      NaN        2.0        2.0  \n",
      "4      NaN        2.0        1.0  \n",
      "\n",
      "[5 rows x 176 columns]\n",
      "      ID1              CASEID  V501  V502  V503  V504  V505  V506  V507  \\\n",
      "0  2019.0        000100201  2   1.0   1.0   1.0   1.0   NaN   NaN   1.0   \n",
      "1  2019.0        000102801  2   2.0   1.0   1.0   1.0   NaN   NaN  12.0   \n",
      "2  2019.0        000102801  6   5.0   2.0   1.0   NaN   NaN   NaN   8.0   \n",
      "3  2019.0        000104801  2   2.0   1.0   1.0   1.0   NaN   NaN  12.0   \n",
      "4  2019.0        000113601  2   2.0   1.0   2.0   1.0   NaN   NaN  12.0   \n",
      "\n",
      "     V508  ...  V743C  V743D  V743E  V743F  V744A  V744B  V744C  V744D  V744E  \\\n",
      "0  2008.0  ...    1.0    2.0    1.0    4.0    0.0    0.0    0.0    0.0    0.0   \n",
      "1  2011.0  ...    2.0    1.0    1.0    2.0    0.0    0.0    0.0    0.0    0.0   \n",
      "2  1984.0  ...    NaN    NaN    NaN    NaN    0.0    0.0    0.0    0.0    0.0   \n",
      "3  2009.0  ...    1.0    1.0    1.0    1.0    0.0    0.0    0.0    0.0    0.0   \n",
      "4  2005.0  ...    1.0    1.0    2.0    4.0    0.0    0.0    0.0    0.0    0.0   \n",
      "\n",
      "   V746  \n",
      "0   1.0  \n",
      "1   2.0  \n",
      "2   NaN  \n",
      "3   NaN  \n",
      "4   2.0  \n",
      "\n",
      "[5 rows x 84 columns]\n"
     ]
    }
   ],
   "source": [
    "print(rec_1.head())\n",
    "print(rec_2.head())\n",
    "print(rec_3.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we start by shortening the term \"pandas\" to \"pd\" while preparing the \"import pyreadstat\" command in order to work with SCSS files in Pandas. Then, we just choose to \"folder\" and \"files\" as variables of the directory and the 3 main files respectively. After this, it is time to import and change the name of files. With this ready, we need to generate new variables (\"var_labels#\" and \"value_labels#\") and assign the content of them. That is all for the step 1, the use of print to the recs variables is only for verification issues. In adittion, the use of \".head\" is to show only a little part of the DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Select the following columns for each data set:\n",
    "|Data|Columns|\n",
    "|---|---|\n",
    "|rec1| CASEID, V000, V001, V002, V003, V004, V007, V008, V009, V010, V011, V012, V024, V102, V120, V121, V122, V123, V124, V125, V127, V133 |\n",
    "|rec2| CASEID, V201, V218, V301, V302, V323, V323A, V325A, V326, V327, V337, V359, V360, V361, V362, V363, V364, V367, V372, V372A, V375A, V376, V376A, V379, V380 |\n",
    "|rec3| CASEID, V501, V502, V503, V504, V505, V506, V507, V508, V509, V510, V511, V512, V513, V525, V613, V714, V715 |\n",
    "\n",
    "\n",
    "Additioanlly, you should update the variables and value labels objects. They must have information only for the selected columns. The new dataframes should be name as `rec1_1`, `rec2_1`, and `rec3_1`. The new varible labels objects should be named as `new_var_labels1`, `new_var_labels2`, and `new_var_labels3`. The new value labels objects should be named as `new_value_labels1`, `new_value_labels2`, and `new_value_labels3` **Hint: Use the `loc` and column names to filter. Update the dictionary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_rec1 = ['CASEID', 'V000', 'V001', 'V002', 'V003', 'V004', 'V007', 'V008', 'V009', 'V010', 'V011', 'V012', 'V024', 'V102', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V127', 'V133']\n",
    "columns_rec2 = ['CASEID', 'V201', 'V218', 'V301', 'V302', 'V323', 'V323A', 'V325A', 'V326', 'V327', 'V337', 'V359', 'V360', 'V361', 'V362', 'V363', 'V364', 'V367', 'V372', 'V372A', 'V375A', 'V376', 'V376A', 'V379', 'V380']\n",
    "columns_rec3 = ['CASEID', 'V501', 'V502', 'V503', 'V504', 'V505', 'V506', 'V507', 'V508', 'V509', 'V510', 'V511', 'V512', 'V513', 'V525', 'V613', 'V714', 'V715']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec1_1 = rec_1[columns_rec1].copy()\n",
    "rec2_1 = rec_2[columns_rec2].copy()\n",
    "rec3_1 = rec_3[columns_rec3].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_var_labels1 = {col: var_labels1.get(col, \"\") for col in columns_rec1}\n",
    "new_var_labels2 = {col: var_labels2.get(col, \"\") for col in columns_rec2}\n",
    "new_var_labels3 = {col: var_labels3.get(col, \"\") for col in columns_rec3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_value_labels1 = {col: value_labels1.get(col, {}) for col in columns_rec1}\n",
    "new_value_labels2 = {col: value_labels2.get(col, {}) for col in columns_rec2}\n",
    "new_value_labels3 = {col: value_labels3.get(col, {}) for col in columns_rec3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1rst step: create variables type \"columns_rec#\" to insert every data in his respective group. 2nd step: create standalone copys (dataframes) of the previous variables and called them as rec#_ #.  3nd step: Create  new variable and value labels through of \"col\" and \".get\" to preservate the original version even if the new variables are modified. In addition, it is relevant to mention these dictionaries contain only main columns of every correpondent data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Generate a new column for `rec1_1` named as `year`. It should be equal to `2019`. Also, you must update this new variable for the `var_labels` dictionary. Generate a new key for `new_var_labels1` and the value for this key should be **\"Year of the survey\"** **Hint: Use `loc` and `update` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec1_1.loc[:, 'year'] = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_var_labels1.update({'year': 'Year of the survey'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1rst step: Generate the new column \"year\" through the \".loc\" method (you just need to write \"year\" and since it is not there, it will be added automatically) . 2nd step: Update the dictionary adding \"Year of the survey\" by means of the \"year\" key. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Merge `rec1_1`, `rec2_1`, and `rec3_1` using **CASEID**. Name this new object as `endes_2019`. **Hint: Use [this link](https://stackoverflow.com/questions/53645882/pandas-merging-101)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               CASEID V000  V001  V002  V003  V004    V007    V008  V009  \\\n",
      "0        000100201  2  PE6   1.0   2.0   2.0   1.0  2019.0  1434.0   4.0   \n",
      "1        000100201  3  PE6   1.0   2.0   3.0   1.0  2019.0  1434.0   1.0   \n",
      "2        000102801  2  PE6   1.0  28.0   2.0   1.0  2019.0  1434.0   6.0   \n",
      "3        000102801  6  PE6   1.0  28.0   6.0   1.0  2019.0  1434.0   3.0   \n",
      "4        000104801  2  PE6   1.0  48.0   2.0   1.0  2019.0  1434.0   5.0   \n",
      "\n",
      "     V010  ...    V508    V509  V510  V511  V512  V513  V525  V613  V714  V715  \n",
      "0  1986.0  ...  2008.0  1297.0   1.0  21.0  11.0   3.0  17.0   2.0   1.0  11.0  \n",
      "1  2007.0  ...     NaN     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "2  1983.0  ...  2011.0  1344.0   1.0  28.0   7.0   2.0  18.0   3.0   1.0  14.0  \n",
      "3  1970.0  ...  1984.0  1016.0   5.0  14.0  34.0   7.0  14.0   0.0   0.0   6.0  \n",
      "4  1991.0  ...  2009.0  1320.0   1.0  18.0   9.0   2.0  15.0   2.0   0.0   6.0  \n",
      "\n",
      "[5 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "#In this step, the pd.merge function is used to combine the three datasets (rec1_1, rec2_1, and rec3_1) into a single DataFrame named endes_2019. The key for merging is the 'CASEID' column, which is common across the three datasets. The how='outer' parameter specifies that the merge should be an outer join, meaning it includes all rows from both datasets, filling in missing values with NaN where necessary.\n",
    "#endes_2019 = pd.merge(rec1_1, rec2_1, on='CASEID', how='outer')\n",
    "#endes_2019 = pd.merge(endes_2019, rec3_1, on='CASEID', how='outer')\n",
    "#This creates a DataFrame where each row corresponds to a unique 'CASEID', and columns are a combination of columns from rec1_1, rec2_1, and rec3_1. The resulting DataFrame, endes_2019, contains information from all three datasets, joined together on the 'CASEID' column.\n",
    "endes_2019 = pd.merge(rec1_1, rec2_1, on='CASEID', how='outer')\n",
    "endes_2019 = pd.merge(endes_2019, rec3_1, on='CASEID', how='outer')\n",
    "print(endes_2019.head())\n",
    "#This line prints the first few rows of the merged DataFrame, endes_2019, to verify that the merge was successful. It allows you to visually inspect the combined data and check if the merging process resulted in the expected structure. The head() method is used to display only the top rows of the DataFrame for a quick overview."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Unify all the `new_var_labels` in one object and `new_value_labels` in another one object. Name these two objects as `var_labels` and `value_labels`. Use them to generate new attributes for `endes_2019`. These attributes should be named as `var_labels` and `value_labels`. **Hint: Use `update` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               CASEID V000  V001  V002  V003  V004    V007    V008  V009  \\\n",
      "0        000100201  2  PE6   1.0   2.0   2.0   1.0  2019.0  1434.0   4.0   \n",
      "1        000100201  3  PE6   1.0   2.0   3.0   1.0  2019.0  1434.0   1.0   \n",
      "2        000102801  2  PE6   1.0  28.0   2.0   1.0  2019.0  1434.0   6.0   \n",
      "3        000102801  6  PE6   1.0  28.0   6.0   1.0  2019.0  1434.0   3.0   \n",
      "4        000104801  2  PE6   1.0  48.0   2.0   1.0  2019.0  1434.0   5.0   \n",
      "\n",
      "     V010  ...  V510  V511  V512  V513  V525  V613  V714  V715  var_labels  \\\n",
      "0  1986.0  ...   1.0  21.0  11.0   3.0  17.0   2.0   1.0  11.0         NaN   \n",
      "1  2007.0  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN         NaN   \n",
      "2  1983.0  ...   1.0  28.0   7.0   2.0  18.0   3.0   1.0  14.0         NaN   \n",
      "3  1970.0  ...   5.0  14.0  34.0   7.0  14.0   0.0   0.0   6.0         NaN   \n",
      "4  1991.0  ...   1.0  18.0   9.0   2.0  15.0   2.0   0.0   6.0         NaN   \n",
      "\n",
      "   value_labels  \n",
      "0           NaN  \n",
      "1           NaN  \n",
      "2           NaN  \n",
      "3           NaN  \n",
      "4           NaN  \n",
      "\n",
      "[5 rows x 66 columns]\n"
     ]
    }
   ],
   "source": [
    "#var_labels and value_labels are created as empty dictionaries.\n",
    "The update method is used to combine the dictionaries new_var_labels1, new_var_labels2, and new_var_labels3 into a single dictionary var_labels. The same process is done for new_value_labels1, new_value_labels2, and new_value_labels3 to create value_labels.\n",
    "Two new attributes, var_labels and value_labels, are added to the endes_2019 DataFrame. These attributes contain the unified information for variable labels and value labels, respectively.\n",
    "# Unifying new_var_labels into var_labels\n",
    "var_labels = {}\n",
    "var_labels.update(new_var_labels1)\n",
    "var_labels.update(new_var_labels2)\n",
    "var_labels.update(new_var_labels3)\n",
    "\n",
    "# Unifying new_value_labels into value_labels\n",
    "value_labels = {}\n",
    "value_labels.update(new_value_labels1)\n",
    "value_labels.update(new_value_labels2)\n",
    "value_labels.update(new_value_labels3)\n",
    "\n",
    "# Adding new attributes to endes_2019\n",
    "endes_2019['var_labels'] = var_labels\n",
    "endes_2019['value_labels'] = value_labels\n",
    "\n",
    "# Displaying a sample of the updated DataFrame\n",
    "print(endes_2019.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Get the min, max, sd, n_obs, n_missing for the following columns **total children ever born (V201)**, **Ideal number of children (V613)**, **Husbands education-single yrs (V715)**, and **Age at first marriage (V511)**. We want a dataframe with the following columns **Variables, Min, Max, Mean, N_obs, N_missing** and sort by the number of missing rows. **Hint: Use `describe` and `pivot` methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Variables   Min   Max       Mean  N_obs  N_missing\n",
      "V715      V715   0.0  98.0  11.624474  25881      12454\n",
      "V511      V511  10.0  48.0  20.463738  25881      12454\n",
      "V613      V613   0.0  96.0   2.455345  33311       5024\n",
      "V201      V201   0.0  13.0   1.781946  36922       1413\n"
     ]
    }
   ],
   "source": [
    "#describe() is used to get summary statistics for the specified columns.\n",
    "#The transpose() method is used to switch rows and columns for easier handling.\n",
    "#The count of missing values is calculated using isnull().sum().\n",
    "#A new DataFrame (result_df) is created with columns Variables, Min, Max, Mean, N_obs, and N_missing.\n",
    "#The DataFrame is sorted by the number of missing rows in descending order.\n",
    "\n",
    "# Specify the columns of interest\n",
    "columns_of_interest = ['V201', 'V613', 'V715', 'V511']\n",
    "\n",
    "# Calculate summary statistics using describe\n",
    "summary_stats = endes_2019[columns_of_interest].describe().transpose()\n",
    "\n",
    "# Get the count of missing values\n",
    "missing_counts = endes_2019[columns_of_interest].isnull().sum()\n",
    "\n",
    "# Create a DataFrame with required columns\n",
    "result_df = pd.DataFrame({\n",
    "    'Variables': summary_stats.index,\n",
    "    'Min': summary_stats['min'],\n",
    "    'Max': summary_stats['max'],\n",
    "    'Mean': summary_stats['mean'],\n",
    "    'N_obs': summary_stats['count'].astype(int),\n",
    "    'N_missing': missing_counts\n",
    "})\n",
    "\n",
    "# Sort by the number of missing rows\n",
    "result_df = result_df.sort_values(by='N_missing', ascending=False)\n",
    "\n",
    "# Display the result\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Use `endes_2019` data to generate a new object named `mean_key_vars` to find the mean of **total children ever born (V201)**, **Ideal number of children (V613)**, **Husbands education-single yrs (V715)**, and **Age at first marriage (V511)** by year and department **(V024)**. Name these columns as **mean_total_children, mean_ideal_children, mean_hb_yr_educ and mean_first_marriage**, respectively. **Hint: Use groupby and [this link](https://stackoverflow.com/questions/40901770/is-there-a-simple-way-to-change-a-column-of-yes-no-to-1-0-in-a-pandas-dataframe).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    year  V024  mean_total_children  mean_ideal_children  mean_hb_yr_educ  \\\n",
      "0   2019   1.0             1.980892             2.519355         9.611379   \n",
      "1   2019   2.0             1.771065             2.383260        11.789534   \n",
      "2   2019   3.0             2.003463             2.391176        11.063779   \n",
      "3   2019   4.0             1.539910             2.441390        13.714898   \n",
      "4   2019   5.0             1.848787             2.209026        11.285419   \n",
      "5   2019   6.0             1.931559             2.545923         9.759358   \n",
      "6   2019   7.0             1.567164             2.313846        13.327586   \n",
      "7   2019   8.0             1.894415             2.100097        11.195938   \n",
      "8   2019   9.0             2.083969             4.008662        11.008974   \n",
      "9   2019  10.0             1.989754             2.591647         9.902874   \n",
      "10  2019  11.0             1.671034             2.494357        12.350254   \n",
      "11  2019  12.0             1.836656             2.199660        11.221154   \n",
      "12  2019  13.0             1.704082             2.431468        10.758584   \n",
      "13  2019  14.0             1.673928             3.168783        11.550098   \n",
      "14  2019  15.0             1.476121             2.277727        13.115052   \n",
      "15  2019  16.0             2.157568             2.476703        10.109375   \n",
      "16  2019  17.0             1.972514             2.526173        12.405550   \n",
      "17  2019  18.0             1.626724             2.052925        12.964120   \n",
      "18  2019  19.0             1.854436             2.526570        12.595635   \n",
      "19  2019  20.0             1.812752             2.681343        11.730695   \n",
      "20  2019  21.0             1.794307             2.119433        11.174870   \n",
      "21  2019  22.0             1.903340             2.493333         9.705545   \n",
      "22  2019  23.0             1.436502             2.013083        12.815937   \n",
      "23  2019  24.0             1.756925             2.469824        11.856468   \n",
      "24  2019  25.0             2.012258             2.388399        10.788566   \n",
      "\n",
      "    mean_first_marriage  \n",
      "0             19.266152  \n",
      "1             20.591581  \n",
      "2             20.064982  \n",
      "3             22.304394  \n",
      "4             20.340228  \n",
      "5             19.610695  \n",
      "6             21.517241  \n",
      "7             20.283154  \n",
      "8             19.780769  \n",
      "9             19.818632  \n",
      "10            20.747208  \n",
      "11            20.683761  \n",
      "12            20.774678  \n",
      "13            20.953831  \n",
      "14            21.946329  \n",
      "15            18.448529  \n",
      "16            19.467449  \n",
      "17            21.640046  \n",
      "18            20.395379  \n",
      "19            20.451737  \n",
      "20            20.398964  \n",
      "21            18.953155  \n",
      "22            22.065095  \n",
      "23            19.409821  \n",
      "24            18.985481  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "#We asumed endes_2019 is our DataFrame\n",
    "\n",
    "#We convert the columns with binary answers from strings to integers\n",
    "\n",
    "binary_columns = ['V201', 'V613', 'V715', 'V511']\n",
    "endes_2019[binary_columns] = endes_2019[binary_columns].replace({'No': 0, 'Si': 1}) \n",
    "\n",
    "#We group by 'year' and 'department (V024), then we use the aggregate function and calculate the mean for each variable\n",
    "\n",
    "grouped_mean = endes_2019.groupby(['year', 'V024']).agg({ \n",
    "    'V201': 'mean',\n",
    "    'V613': 'mean',\n",
    "    'V715': 'mean',\n",
    "    'V511': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "#Then we rename the columns as indicates in the instructions \n",
    "grouped_mean.columns = ['year', 'V024', 'mean_total_children', 'mean_ideal_children', 'mean_hb_yr_educ', 'mean_first_marriage']\n",
    "\n",
    "#Then we create the new DataFrame 'mean_key_vars' \n",
    "mean_key_vars = grouped_mean\n",
    "\n",
    "#We print the result\n",
    "\n",
    "print(mean_key_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Reshape `mean_key_vars` from wide to long. We want a dataframe with three columns **dpto, variables, values**. Name this object as `reshape_mean_key_vars`. **Hint: Use melt method**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    year  dpto            variables     values\n",
      "0   2019   1.0  mean_total_children   1.980892\n",
      "1   2019   2.0  mean_total_children   1.771065\n",
      "2   2019   3.0  mean_total_children   2.003463\n",
      "3   2019   4.0  mean_total_children   1.539910\n",
      "4   2019   5.0  mean_total_children   1.848787\n",
      "..   ...   ...                  ...        ...\n",
      "95  2019  21.0  mean_first_marriage  20.398964\n",
      "96  2019  22.0  mean_first_marriage  18.953155\n",
      "97  2019  23.0  mean_first_marriage  22.065095\n",
      "98  2019  24.0  mean_first_marriage  19.409821\n",
      "99  2019  25.0  mean_first_marriage  18.985481\n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#We use the \"melt method\" to help us transform the DataFrame from wide to long format\n",
    "\n",
    "reshape_mean_key_vars = pd.melt(mean_key_vars, id_vars=['year', 'V024'],\n",
    "                               var_name='variables', value_name='values')\n",
    "\n",
    "#In this method we use the 'id_vars' to indicatw what columns we want to remaind unchanged like 'year' and 'dpto (V024)'\n",
    "#Then, we use 'var_name' as a name of the new column that contain the variable names\n",
    "#Finally, 'value_name is the name of the new colum that will contain the values \n",
    "\n",
    "#We rename the columns \n",
    "reshape_mean_key_vars.columns = ['year', 'dpto', 'variables', 'values']\n",
    "\n",
    "#We print the result\n",
    "print(reshape_mean_key_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Replicate your answers for questions 7 and 8, but in one line of code. Make it the most simple as possible. **NO HINT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Merge `reshape_mean_key_vars` with `endes_2019`. Name this object `final_result`. **Hint: Use merge.**"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
