{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the `REC0111.sav`, `RE223132.sav` and `RE516171.sav` files and their variables and values labels from this path `\"../../_data/endes/2019\"`. The name of imported files should be named as `rec_1`, `rec_2` and `rec_3` for files `REC0111.sav`, `RE223132.sav` and `RE516171.sav` respectively. The name of the variable and value labels should be `var_labels1` and `value_labels1` for `rec1`, `var_labels2` and `value_labels2` for `rec2`, and `var_labels3` and `value_labels3` for `rec3`. **Hint: See the section 3.3.4 of [the lecture 3](https://github.com/alexanderquispe/Diplomado_PUCP/blob/main/Lecture_3/Lecture_3.ipynb)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyreadstat\n",
      "  Obtaining dependency information for pyreadstat from https://files.pythonhosted.org/packages/b4/16/20564e8afcdf0c36519a503fdfca86b886bda2b8dc436333e8e8b8e8d8f4/pyreadstat-1.2.6-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading pyreadstat-1.2.6-cp311-cp311-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pandas>=1.2.0 in d:\\programas\\anaconda\\lib\\site-packages (from pyreadstat) (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\programas\\anaconda\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\programas\\anaconda\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\programas\\anaconda\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in d:\\programas\\anaconda\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programas\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->pyreadstat) (1.16.0)\n",
      "Downloading pyreadstat-1.2.6-cp311-cp311-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/2.4 MB 991.0 kB/s eta 0:00:03\n",
      "   ------- -------------------------------- 0.5/2.4 MB 4.7 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 9.5 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 11.8 MB/s eta 0:00:00\n",
      "Installing collected packages: pyreadstat\n",
      "Successfully installed pyreadstat-1.2.6\n"
     ]
    }
   ],
   "source": [
    "!pip install pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"../../_data/endes/2019/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"REC0111.sav\", \"RE223132.sav\", \"RE516171.sav\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_1, meta_1 = pyreadstat.read_sav(f\"{folder}{files[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_2, meta_2 = pyreadstat.read_sav(f\"{folder}{files[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_3, meta_3 = pyreadstat.read_sav(f\"{folder}{files[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_labels1 = meta_1.column_names_to_labels\n",
    "value_labels1 = meta_1.variable_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_labels2 = meta_2.column_names_to_labels\n",
    "value_labels2 = meta_2.variable_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_labels3 = meta_3.column_names_to_labels\n",
    "value_labels3 = meta_3.variable_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ID1             HHID              CASEID  V001  V002  V003  V004  \\\n",
      "0  2019.0        000100201        000100201  2   1.0   2.0   2.0   1.0   \n",
      "1  2019.0        000100201        000100201  3   1.0   2.0   3.0   1.0   \n",
      "2  2019.0        000102801        000102801  2   1.0  28.0   2.0   1.0   \n",
      "3  2019.0        000102801        000102801  6   1.0  28.0   6.0   1.0   \n",
      "4  2019.0        000104801        000104801  2   1.0  48.0   2.0   1.0   \n",
      "\n",
      "     V007    V008  V009  ...  QD333_4  QD333_5  QD333_6  UBIGEO  V022  \\\n",
      "0  2019.0  1434.0   4.0  ...      2.0      2.0      2.0  010101   3.0   \n",
      "1  2019.0  1434.0   1.0  ...      2.0      2.0      2.0  010101   3.0   \n",
      "2  2019.0  1434.0   6.0  ...      2.0      2.0      2.0  010101   3.0   \n",
      "3  2019.0  1434.0   3.0  ...      2.0      2.0      2.0  010101   3.0   \n",
      "4  2019.0  1434.0   5.0  ...      2.0      2.0      2.0  010101   3.0   \n",
      "\n",
      "       V005  V190      V191  mujeres12a49  NCONGLOME  \n",
      "0  154803.0   4.0  1.234450           2.0     7065.0  \n",
      "1  154803.0   4.0  1.234450           0.0     7065.0  \n",
      "2  154803.0   4.0  1.295611           2.0     7065.0  \n",
      "3  154803.0   4.0  1.295611           2.0     7065.0  \n",
      "4  154803.0   2.0 -0.256431           2.0     7065.0  \n",
      "\n",
      "[5 rows x 105 columns]\n",
      "      ID1              CASEID  V201  V202  V203  V204  V205  V206  V207  V208  \\\n",
      "0  2019.0        000100201  2   2.0   1.0   1.0   0.0   0.0   0.0   0.0   1.0   \n",
      "1  2019.0        000100201  3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "2  2019.0        000102801  2   3.0   1.0   2.0   0.0   0.0   0.0   0.0   2.0   \n",
      "3  2019.0        000102801  6   3.0   1.0   0.0   2.0   0.0   0.0   0.0   0.0   \n",
      "4  2019.0        000104801  2   3.0   1.0   2.0   0.0   0.0   0.0   0.0   1.0   \n",
      "\n",
      "   ...  V307_09  V307_10  V307_11  V307_12  V307_13  V307_14  V307_15  \\\n",
      "0  ...      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
      "1  ...      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
      "2  ...      0.0      0.0      NaN      NaN      NaN      NaN      NaN   \n",
      "3  ...      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
      "4  ...      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
      "\n",
      "   V307_16  QI302_05A  QI302_05B  \n",
      "0      0.0        1.0        2.0  \n",
      "1      NaN        NaN        NaN  \n",
      "2      0.0        2.0        1.0  \n",
      "3      NaN        2.0        2.0  \n",
      "4      NaN        2.0        1.0  \n",
      "\n",
      "[5 rows x 176 columns]\n",
      "      ID1              CASEID  V501  V502  V503  V504  V505  V506  V507  \\\n",
      "0  2019.0        000100201  2   1.0   1.0   1.0   1.0   NaN   NaN   1.0   \n",
      "1  2019.0        000102801  2   2.0   1.0   1.0   1.0   NaN   NaN  12.0   \n",
      "2  2019.0        000102801  6   5.0   2.0   1.0   NaN   NaN   NaN   8.0   \n",
      "3  2019.0        000104801  2   2.0   1.0   1.0   1.0   NaN   NaN  12.0   \n",
      "4  2019.0        000113601  2   2.0   1.0   2.0   1.0   NaN   NaN  12.0   \n",
      "\n",
      "     V508  ...  V743C  V743D  V743E  V743F  V744A  V744B  V744C  V744D  V744E  \\\n",
      "0  2008.0  ...    1.0    2.0    1.0    4.0    0.0    0.0    0.0    0.0    0.0   \n",
      "1  2011.0  ...    2.0    1.0    1.0    2.0    0.0    0.0    0.0    0.0    0.0   \n",
      "2  1984.0  ...    NaN    NaN    NaN    NaN    0.0    0.0    0.0    0.0    0.0   \n",
      "3  2009.0  ...    1.0    1.0    1.0    1.0    0.0    0.0    0.0    0.0    0.0   \n",
      "4  2005.0  ...    1.0    1.0    2.0    4.0    0.0    0.0    0.0    0.0    0.0   \n",
      "\n",
      "   V746  \n",
      "0   1.0  \n",
      "1   2.0  \n",
      "2   NaN  \n",
      "3   NaN  \n",
      "4   2.0  \n",
      "\n",
      "[5 rows x 84 columns]\n"
     ]
    }
   ],
   "source": [
    "print(rec_1.head())\n",
    "print(rec_2.head())\n",
    "print(rec_3.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we start by shortening the term \"pandas\" to \"pd\" while preparing the \"import pyreadstat\" command in order to work with SCSS files in Pandas. Then, we just choose to \"folder\" and \"files\" as variables of the directory and the 3 main files respectively. After this, it is time to import and change the name of files. With this ready, we need to generate new variables (\"var_labels#\" and \"value_labels#\") and assign the content of them. That is all for the step 1, the use of print to the recs variables is only for verification issues. In adittion, the use of \".head\" is to show only a little part of the DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Select the following columns for each data set:\n",
    "|Data|Columns|\n",
    "|---|---|\n",
    "|rec1| CASEID, V000, V001, V002, V003, V004, V007, V008, V009, V010, V011, V012, V024, V102, V120, V121, V122, V123, V124, V125, V127, V133 |\n",
    "|rec2| CASEID, V201, V218, V301, V302, V323, V323A, V325A, V326, V327, V337, V359, V360, V361, V362, V363, V364, V367, V372, V372A, V375A, V376, V376A, V379, V380 |\n",
    "|rec3| CASEID, V501, V502, V503, V504, V505, V506, V507, V508, V509, V510, V511, V512, V513, V525, V613, V714, V715 |\n",
    "\n",
    "\n",
    "Additioanlly, you should update the variables and value labels objects. They must have information only for the selected columns. The new dataframes should be name as `rec1_1`, `rec2_1`, and `rec3_1`. The new varible labels objects should be named as `new_var_labels1`, `new_var_labels2`, and `new_var_labels3`. The new value labels objects should be named as `new_value_labels1`, `new_value_labels2`, and `new_value_labels3` **Hint: Use the `loc` and column names to filter. Update the dictionary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_rec1 = ['CASEID', 'V000', 'V001', 'V002', 'V003', 'V004', 'V007', 'V008', 'V009', 'V010', 'V011', 'V012', 'V024', 'V102', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V127', 'V133']\n",
    "columns_rec2 = ['CASEID', 'V201', 'V218', 'V301', 'V302', 'V323', 'V323A', 'V325A', 'V326', 'V327', 'V337', 'V359', 'V360', 'V361', 'V362', 'V363', 'V364', 'V367', 'V372', 'V372A', 'V375A', 'V376', 'V376A', 'V379', 'V380']\n",
    "columns_rec3 = ['CASEID', 'V501', 'V502', 'V503', 'V504', 'V505', 'V506', 'V507', 'V508', 'V509', 'V510', 'V511', 'V512', 'V513', 'V525', 'V613', 'V714', 'V715']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec1_1 = rec_1[columns_rec1].copy()\n",
    "rec2_1 = rec_2[columns_rec2].copy()\n",
    "rec3_1 = rec_3[columns_rec3].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_var_labels1 = {col: var_labels1.get(col, \"\") for col in columns_rec1}\n",
    "new_var_labels2 = {col: var_labels2.get(col, \"\") for col in columns_rec2}\n",
    "new_var_labels3 = {col: var_labels3.get(col, \"\") for col in columns_rec3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_value_labels1 = {col: value_labels1.get(col, {}) for col in columns_rec1}\n",
    "new_value_labels2 = {col: value_labels2.get(col, {}) for col in columns_rec2}\n",
    "new_value_labels3 = {col: value_labels3.get(col, {}) for col in columns_rec3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1rst step: create variables type \"columns_rec#\" to insert every data in his respective group. 2nd step: create standalone copys (dataframes) of the previous variables and called them as rec#_ #.  3nd step: Create  new variable and value labels through of \"col\" and \".get\" to preservate the original version even if the new variables are modified. In addition, it is relevant to mention these dictionaries contain only main columns of every correpondent data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Generate a new column for `rec1_1` named as `year`. It should be equal to `2019`. Also, you must update this new variable for the `var_labels` dictionary. Generate a new key for `new_var_labels1` and the value for this key should be **\"Year of the survey\"** **Hint: Use `loc` and `update` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec1_1.loc[:, 'year'] = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_var_labels1.update({'year': 'Year of the survey'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1rst step: Generate the new column \"year\" through the \".loc\" method (you just need to write \"year\" and since it is not there, it will be added automatically) . 2nd step: Update the dictionary adding \"Year of the survey\" by means of the \"year\" key. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Merge `rec1_1`, `rec2_1`, and `rec3_1` using **CASEID**. Name this new object as `endes_2019`. **Hint: Use [this link](https://stackoverflow.com/questions/53645882/pandas-merging-101)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               CASEID V000  V001  V002  V003  V004    V007    V008  V009  \\\n",
      "0        000100201  2  PE6   1.0   2.0   2.0   1.0  2019.0  1434.0   4.0   \n",
      "1        000100201  3  PE6   1.0   2.0   3.0   1.0  2019.0  1434.0   1.0   \n",
      "2        000102801  2  PE6   1.0  28.0   2.0   1.0  2019.0  1434.0   6.0   \n",
      "3        000102801  6  PE6   1.0  28.0   6.0   1.0  2019.0  1434.0   3.0   \n",
      "4        000104801  2  PE6   1.0  48.0   2.0   1.0  2019.0  1434.0   5.0   \n",
      "\n",
      "     V010  ...    V508    V509  V510  V511  V512  V513  V525  V613  V714  V715  \n",
      "0  1986.0  ...  2008.0  1297.0   1.0  21.0  11.0   3.0  17.0   2.0   1.0  11.0  \n",
      "1  2007.0  ...     NaN     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "2  1983.0  ...  2011.0  1344.0   1.0  28.0   7.0   2.0  18.0   3.0   1.0  14.0  \n",
      "3  1970.0  ...  1984.0  1016.0   5.0  14.0  34.0   7.0  14.0   0.0   0.0   6.0  \n",
      "4  1991.0  ...  2009.0  1320.0   1.0  18.0   9.0   2.0  15.0   2.0   0.0   6.0  \n",
      "\n",
      "[5 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "#In this step, the pd.merge function is used to combine the three datasets (rec1_1, rec2_1, and rec3_1) into a single DataFrame named endes_2019. The key for merging is the 'CASEID' column, which is common across the three datasets. The how='outer' parameter specifies that the merge should be an outer join, meaning it includes all rows from both datasets, filling in missing values with NaN where necessary.\n",
    "#endes_2019 = pd.merge(rec1_1, rec2_1, on='CASEID', how='outer')\n",
    "#endes_2019 = pd.merge(endes_2019, rec3_1, on='CASEID', how='outer')\n",
    "#This creates a DataFrame where each row corresponds to a unique 'CASEID', and columns are a combination of columns from rec1_1, rec2_1, and rec3_1. The resulting DataFrame, endes_2019, contains information from all three datasets, joined together on the 'CASEID' column.\n",
    "endes_2019 = pd.merge(rec1_1, rec2_1, on='CASEID', how='outer')\n",
    "endes_2019 = pd.merge(endes_2019, rec3_1, on='CASEID', how='outer')\n",
    "print(endes_2019.head())\n",
    "#This line prints the first few rows of the merged DataFrame, endes_2019, to verify that the merge was successful. It allows you to visually inspect the combined data and check if the merging process resulted in the expected structure. The head() method is used to display only the top rows of the DataFrame for a quick overview."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Unify all the `new_var_labels` in one object and `new_value_labels` in another one object. Name these two objects as `var_labels` and `value_labels`. Use them to generate new attributes for `endes_2019`. These attributes should be named as `var_labels` and `value_labels`. **Hint: Use `update` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3621246572.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[24], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    The update method is used to combine the dictionaries new_var_labels1, new_var_labels2, and new_var_labels3 into a single dictionary var_labels. The same process is done for new_value_labels1, new_value_labels2, and new_value_labels3 to create value_labels.\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#var_labels and value_labels are created as empty dictionaries.\n",
    "The update method is used to combine the dictionaries new_var_labels1, new_var_labels2, and new_var_labels3 into a single dictionary var_labels. The same process is done for new_value_labels1, new_value_labels2, and new_value_labels3 to create value_labels.\n",
    "Two new attributes, var_labels and value_labels, are added to the endes_2019 DataFrame. These attributes contain the unified information for variable labels and value labels, respectively.\n",
    "# Unifying new_var_labels into var_labels\n",
    "var_labels = {}\n",
    "var_labels.update(new_var_labels1)\n",
    "var_labels.update(new_var_labels2)\n",
    "var_labels.update(new_var_labels3)\n",
    "\n",
    "# Unifying new_value_labels into value_labels\n",
    "value_labels = {}\n",
    "value_labels.update(new_value_labels1)\n",
    "value_labels.update(new_value_labels2)\n",
    "value_labels.update(new_value_labels3)\n",
    "\n",
    "# Adding new attributes to endes_2019\n",
    "endes_2019['var_labels'] = var_labels\n",
    "endes_2019['value_labels'] = value_labels\n",
    "\n",
    "# Displaying a sample of the updated DataFrame\n",
    "print(endes_2019.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Get the min, max, sd, n_obs, n_missing for the following columns **total children ever born (V201)**, **Ideal number of children (V613)**, **Husbands education-single yrs (V715)**, and **Age at first marriage (V511)**. We want a dataframe with the following columns **Variables, Min, Max, Mean, N_obs, N_missing** and sort by the number of missing rows. **Hint: Use `describe` and `pivot` methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#describe() is used to get summary statistics for the specified columns.\n",
    "#The transpose() method is used to switch rows and columns for easier handling.\n",
    "#The count of missing values is calculated using isnull().sum().\n",
    "#A new DataFrame (result_df) is created with columns Variables, Min, Max, Mean, N_obs, and N_missing.\n",
    "#The DataFrame is sorted by the number of missing rows in descending order.\n",
    "\n",
    "# Specify the columns of interest\n",
    "columns_of_interest = ['V201', 'V613', 'V715', 'V511']\n",
    "\n",
    "# Calculate summary statistics using describe\n",
    "summary_stats = endes_2019[columns_of_interest].describe().transpose()\n",
    "\n",
    "# Get the count of missing values\n",
    "missing_counts = endes_2019[columns_of_interest].isnull().sum()\n",
    "\n",
    "# Create a DataFrame with required columns\n",
    "result_df = pd.DataFrame({\n",
    "    'Variables': summary_stats.index,\n",
    "    'Min': summary_stats['min'],\n",
    "    'Max': summary_stats['max'],\n",
    "    'Mean': summary_stats['mean'],\n",
    "    'N_obs': summary_stats['count'].astype(int),\n",
    "    'N_missing': missing_counts\n",
    "})\n",
    "\n",
    "# Sort by the number of missing rows\n",
    "result_df = result_df.sort_values(by='N_missing', ascending=False)\n",
    "\n",
    "# Display the result\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Use `endes_2019` data to generate a new object named `mean_key_vars` to find the mean of **total children ever born (V201)**, **Ideal number of children (V613)**, **Husbands education-single yrs (V715)**, and **Age at first marriage (V511)** by year and department **(V024)**. Name these columns as **mean_total_children, mean_ideal_children, mean_hb_yr_educ and mean_first_marriage**, respectively. **Hint: Use groupby and [this link](https://stackoverflow.com/questions/40901770/is-there-a-simple-way-to-change-a-column-of-yes-no-to-1-0-in-a-pandas-dataframe).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "#We asumed endes_2019 is our DataFrame\n",
    "\n",
    "#We convert the columns with binary answers from strings to integers\n",
    "\n",
    "binary_columns = ['V201', 'V613', 'V715', 'V511']\n",
    "endes_2019[binary_columns] = endes_2019[binary_columns].replace({'No': 0, 'Si': 1}) \n",
    "\n",
    "#We group by 'year' and 'department (V024), then we use the aggregate function and calculate the mean for each variable\n",
    "\n",
    "grouped_mean = endes_2019.groupby(['year', 'V024']).agg({ \n",
    "    'V201': 'mean',\n",
    "    'V613': 'mean',\n",
    "    'V715': 'mean',\n",
    "    'V511': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "#Then we rename the columns as indicates in the instructions \n",
    "grouped_mean.columns = ['year', 'V024', 'mean_total_children', 'mean_ideal_children', 'mean_hb_yr_educ', 'mean_first_marriage']\n",
    "\n",
    "#Then we create the new DataFrame 'mean_key_vars' \n",
    "mean_key_vars = grouped_mean\n",
    "\n",
    "#We print the result\n",
    "\n",
    "print(mean_key_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Reshape `mean_key_vars` from wide to long. We want a dataframe with three columns **dpto, variables, values**. Name this object as `reshape_mean_key_vars`. **Hint: Use melt method**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use the \"melt method\" to help us transform the DataFrame from wide to long format\n",
    "\n",
    "reshape_mean_key_vars = pd.melt(mean_key_vars, id_vars=['year', 'V024'],\n",
    "                               var_name='variables', value_name='values')\n",
    "\n",
    "#In this method we use the 'id_vars' to indicatw what columns we want to remaind unchanged like 'year' and 'dpto (V024)'\n",
    "#Then, we use 'var_name' as a name of the new column that contain the variable names\n",
    "#Finally, 'value_name is the name of the new colum that will contain the values \n",
    "\n",
    "#We rename the columns \n",
    "reshape_mean_key_vars.columns = ['year', 'dpto', 'variables', 'values']\n",
    "\n",
    "#We print the result\n",
    "print(reshape_mean_key_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Replicate your answers for questions 7 and 8, but in one line of code. Make it the most simple as possible. **NO HINT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    year  V024 variables     values\n",
      "0   2019   1.0      V201   1.980892\n",
      "1   2019   2.0      V201   1.771065\n",
      "2   2019   3.0      V201   2.003463\n",
      "3   2019   4.0      V201   1.539910\n",
      "4   2019   5.0      V201   1.848787\n",
      "..   ...   ...       ...        ...\n",
      "95  2019  21.0      V511  20.398964\n",
      "96  2019  22.0      V511  18.953155\n",
      "97  2019  23.0      V511  22.065095\n",
      "98  2019  24.0      V511  19.409821\n",
      "99  2019  25.0      V511  18.985481\n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# We use the groupby method to group the DataFrame 'endes_2019' by year ('year') and department ('V024'). \n",
    "# Then, we use agg to calculate the mean ('mean') of specific variables ('V201', 'V613', 'V715', 'V511'). \n",
    "# After that, we use reset_index() to reset the indices and obtain a flat DataFrame. \n",
    "# Finally, we use the pandas melt method to transform the DataFrame from wide to long format.\"\n",
    "\n",
    "reshape_mean_key_vars = pd.melt(endes_2019.groupby(['year', 'V024']).agg({'V201': 'mean', 'V613': 'mean', 'V715': 'mean', 'V511': 'mean'}).reset_index(), id_vars=['year', 'V024'], var_name='variables', value_name='values')\n",
    "\n",
    "print(reshape_mean_key_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Merge `reshape_mean_key_vars` with `endes_2019`. Name this object `final_result`. **Hint: Use merge.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    CASEID V000    V001  V002  V003    V004    V007    V008  \\\n",
      "0             000100201  2  PE6     1.0   2.0   2.0     1.0  2019.0  1434.0   \n",
      "1             000100201  2  PE6     1.0   2.0   2.0     1.0  2019.0  1434.0   \n",
      "2             000100201  2  PE6     1.0   2.0   2.0     1.0  2019.0  1434.0   \n",
      "3             000100201  2  PE6     1.0   2.0   2.0     1.0  2019.0  1434.0   \n",
      "4             000100201  3  PE6     1.0   2.0   3.0     1.0  2019.0  1434.0   \n",
      "...                    ...  ...     ...   ...   ...     ...     ...     ...   \n",
      "153335        325407201  2  PE6  3254.0  72.0   2.0  3254.0  2019.0  1440.0   \n",
      "153336        325407401  2  PE6  3254.0  74.0   2.0  3254.0  2019.0  1440.0   \n",
      "153337        325407401  2  PE6  3254.0  74.0   2.0  3254.0  2019.0  1440.0   \n",
      "153338        325407401  2  PE6  3254.0  74.0   2.0  3254.0  2019.0  1440.0   \n",
      "153339        325407401  2  PE6  3254.0  74.0   2.0  3254.0  2019.0  1440.0   \n",
      "\n",
      "        V009    V010  ...  V510  V511  V512  V513  V525  V613  V714  V715  \\\n",
      "0        4.0  1986.0  ...   1.0  21.0  11.0   3.0  17.0   2.0   1.0  11.0   \n",
      "1        4.0  1986.0  ...   1.0  21.0  11.0   3.0  17.0   2.0   1.0  11.0   \n",
      "2        4.0  1986.0  ...   1.0  21.0  11.0   3.0  17.0   2.0   1.0  11.0   \n",
      "3        4.0  1986.0  ...   1.0  21.0  11.0   3.0  17.0   2.0   1.0  11.0   \n",
      "4        1.0  2007.0  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "...      ...     ...  ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
      "153335  12.0  1994.0  ...   1.0  13.0  11.0   3.0  13.0   2.0   0.0  11.0   \n",
      "153336  10.0  1996.0  ...   1.0  16.0   6.0   2.0  15.0   2.0   0.0  11.0   \n",
      "153337  10.0  1996.0  ...   1.0  16.0   6.0   2.0  15.0   2.0   0.0  11.0   \n",
      "153338  10.0  1996.0  ...   1.0  16.0   6.0   2.0  15.0   2.0   0.0  11.0   \n",
      "153339  10.0  1996.0  ...   1.0  16.0   6.0   2.0  15.0   2.0   0.0  11.0   \n",
      "\n",
      "        variables     values  \n",
      "0            V201   1.980892  \n",
      "1            V613   2.519355  \n",
      "2            V715   9.611379  \n",
      "3            V511  19.266152  \n",
      "4            V201   1.980892  \n",
      "...           ...        ...  \n",
      "153335       V511  18.985481  \n",
      "153336       V201   2.012258  \n",
      "153337       V613   2.388399  \n",
      "153338       V715  10.788566  \n",
      "153339       V511  18.985481  \n",
      "\n",
      "[153340 rows x 66 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# We use the merge function to combine the dataset endes_2019 with the object reshape_mean_key_vars\n",
    "\n",
    "final_result = pd.merge(endes_2019, reshape_mean_key_vars, left_on=['year', 'V024'], right_on=['year', 'V024'], how='inner')\n",
    "\n",
    "print(final_result)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
